\chapter{Evaluation}

In the previous chapter we proposed an algorithm that finds an efficient distribution of any given quantum circuit. Additionally, we have given two extensions (\S\ref{pullCNOTs} and \S\ref{BothEnds}) that allow the algorithm to apply some extra optimisations. In this chapter, we will evaluate the distributed circuits our algorithm generates. The quantum programs we distribute are examples drawn from the literature of what we may want to run on quantum computers.

The code of our algorithm's implementation can be found at {\small \url{https://github.com/PabloAndresMartinez/Distributed}}, along with the generated executables and all the data discussed in this chapter. 


\section{Implementation details}
\label{implementation}

The algorithm described in \S\ref{Vanilla} and its two extensions (\texttt{SlideCNOTs} \S\ref{pullCNOTs} and \texttt{EitherRemote} \S\ref{BothEnds}) have been implemented in Haskell. Quipper, a quantum circuit description language, is embedded on Haskell. The quantum programs we use in our evaluation (see \S\ref{benchmark}) are all available as part of the Quipper system, so by using Haskell we could easily manage them. The hypergraph partitioning problem is solved by a specialised third-party software, \texttt{KaHyPart} \citep{KaHyPart}, that is called by our program when needed. A brief overview of \texttt{KaHyPart} is given in Appendix~\ref{chap:HypPart}. 

The main contributions of this thesis were described in Chapter~\ref{chap:Project}. Our implementation is only meant as a demonstration of our algorithm, and it is intended for evaluation purposes only. Thus, efficiency was not a main concern. Nevertheless, it is efficient enough to manage circuits with up to \(50,000\) CNOT gates within a reasonable amount of resources (see \S\ref{Results}).

Our implementation receives a circuit described in Quipper's internal data structure and outputs its distributed version, also in Quipper's format. Thus, it can be integrated inside any Quipper program. However, along the process the circuit is managed as a list of gates, rather than using Quipper's internal data structure. This is the main cause of inefficiencies in our code. Quipper is presented to its users as a language to define circuits, rather than a language to define circuit transformations. Therefore, there are not enough functionalities available at user-level to comfortably implement our algorithm within Quipper. If we intended to achieve better efficiency, we would likely need to extend the internal workings -- the back-end -- of Quipper, which is beyond the scope of this thesis.

Once our input circuit is converted\footnote{This conversion and its backwards counterpart are provided by Quipper.} from Quipper's data structure to a list of gates, implementing our algorithm is straight-forward. Depending on two input flags, the extensions \texttt{SlideCNOTs} and \texttt{EitherRemote} are applied or not; and the parameters \((k,\epsilon)\) -- number of QPUs and load-imbalance tolerance -- are also given as input. Throughout this chapter we use a small load-imbalance tolerance, chosen arbitrarily to be \(\varepsilon = 0.03\). The hypergraph generated by Algorithm~\ref{code:buildHypVanilla} (or Algorithm~\ref{code:buildHypBothEnds}) is written in a file using \texttt{KaHyPart}'s format, and the resulting partition is read from \texttt{KaHyPart}'s output file and used to create the distributed circuit as described in \S\ref{Vanilla}.


\section{Test suite}
\label{benchmark}

The quantum programs whose circuits we will input to our algorithm are available as part of the Quipper system. At the time of writing this thesis, the Quipper system provides seven quantum programs, each with their own default configuration parameters. We use four of these seven examples, with their default configuration unless stated otherwise. The three programs we omit in our evaluation either lack an explicit (gate by gate) implementation of some fragment of their circuits, or their size is beyond the capabilities of our implementation (see \S\ref{Results}). Detailed information about each of these quantum programs can be found in Quipper's online documentation\footnote{Quipper's documentation: \url{https://www.mathstat.dal.ca/~selinger/quipper/doc/}.}. The four programs we consider are the following:

\begin{itemize}
\item \textit{Boolean Formula (BF)}: \citet{BFWalk} showed that the problem of evaluating a boolean formula over \(N\) variables could be solved in time \(\sqrt{N}\) on a quantum computer. We consider the circuit implementing the main part of the algorithm -- the quantum walk. Implemented on Quipper by A. Green.

\item \textit{Binary Welded Tree (BWT)}: \citet{BWT} proposed the problem of finding a path between two nodes in a particular kind of graph (a binary welded tree), and gave a quantum program that solves it exponentially faster than any known classical algorithm. We consider the circuit implementing the overall algorithm, making the tree height twice as large as the default input. Implemented on Quipper by P. Selinger and B. Valiron.

\item \textit{Ground State Estimation (GSE)}: \citet{GSE} proposed how to efficiently calculate the energy of a molecular system's ground state, which is relevant in chemistry. We consider the circuit implementing the overall algorithm. We doubled the default number of basis functions and the number of occupied orbitals. Implemented on Quipper by A. Green et al.

\item \textit{Unique Shortest Vector (USV)}: \citet{USV} proposed a problem where some characteristic vector of an input lattice must be found. This problem requires a large amount of resources, so we only consider a part of it, labelled `R' in Quipper's library. We reduced the default dimension of the lattice from \(5\) to \(2\). Implemented on Quipper by N. Ross.
\end{itemize}

Apart from these four programs, we will include in our test suite the circuit for the \textit{Quantum Fourier Transform (QFT)}. The QFT is a key component in many quantum algorithms, such as Shor's factorisation algorithm. Its implementation is also provided in Quipper's libraries. We will consider multiple QFT circuits of different sizes, referring to the one that uses \(N\!+\!1\) qubits as QFT-\(N\).

\section{Results}
\label{Results}

Our code was compiled using GHC version 8.0.2, with the optimisation flag \texttt{-O2} active. The tests we discuss in this section were run each on a single core of a i7-4710HQ processor, with 8GB of available RAM, on an Ubuntu 16.04 machine. The time it takes to distribute a given circuit mainly depends on the number of CNOT gates it has. Table~\ref{tab:time} shows the number of CNOTs of each circuit and how long it takes to run our algorithm on them. Beyond a circuit size of around \(50,000\) CNOTs, our implementation takes quite long to run. This is because our code was not implemented with scalability as a concern, and not a characteristic of the algorithm itself.

\begin{table}
\caption{CNOT count and the CPU time it takes to distribute each of the circuits discussed in this section. The data shown corresponds to executions with both extensions \texttt{pullCNOTs} and \texttt{EitherRemote} enabled and using distribution parameters \(k\!=\!5,\ \varepsilon\!=\!5\).}
\label{tab:time}
\centering
\vspace*{3mm}
\begin{tabular}{|c|r|r|}
\hline
\textit{Circuit} & \textit{CNOTs} & \textit{Time (s)} \\
\hline
\small
{\small BF} & {\small 25,590} & {\small 64} \\
{\small BWT} & {\small 13,824} & {\small 15} \\
{\small GSE} & {\small 70,158} & {\small 675} \\
{\small USV} & {\small 101,806} & {\small 1,273}\\
\hline
\end{tabular}
\hspace{10mm}
\begin{tabular}{|c|r|r|}
\hline
\textit{Circuit} & \textit{CNOTs} & \textit{Time (s)} \\
\hline
{\small QFT-\(10\)} & {\small 450} & {\small 0.6} \\
{\small QFT-\(25\)} & {\small 3,000} & {\small 3} \\
{\small QFT-\(50\)} & {\small 12,250} & {\small 17} \\
{\small QFT-\(100\)} & {\small 49,500} & {\small 255} \\
{\small QFT-\(200\)} & {\small 199,000} & {\small 5,253} \\
\hline
\end{tabular}
\end{table}

Figure~\ref{fig:results1} shows, for different circuits, the proportion of CNOTs that become non-local once the circuit is partitioned. We also display the proportion of ebits the circuit requires. The key idea behind our algorithm was to take advantage of the remote-control and/or remote-target methods to implement multiple CNOTs using a single ebit. The larger the difference between the two proportions shown in Figure~\ref{fig:results1} is, the more successful this idea has been. However, the definitive measure of success -- and what our algorithm aims to optimise -- is the proportion of ebits itself. 

\input{Figures/Results/1}

Each of the four bars shown for every circuit corresponds to a different configuration of active extensions (from left to right): the vanilla algorithm, only \texttt{SlideCNOTs} enabled, only \texttt{EitherRemote} enabled and both extensions enabled. Among the four configurations, the best one is that whose proportion of ebits is the lowest\footnote{Notice that for a given circuit, the number of CNOTs is independent of the extensions enabled. Thus, the number of ebits is normalised by the same value for each of the four configurations.}. Interestingly, in cases such as BWT when \texttt{SlideCNOTs} is enabled, the algorithm finds a more efficient solution by partitioning the circuit in a way that increases the amount of non-local CNOTs, but ultimately reduces the number of ebits.

As discussed in Remarks~\ref{thm:SlideCNOTsImprove} and~\ref{thm:BothEndsImprove}, in theory, enabling extensions should never increase the ebit count. However, Figure~\ref{fig:results1} proves it otherwise in practice. This is because we were assuming that the optimal solution to the hypergraph partitioning problem was always found, but this is not the case in practice, most notably for QFT-\(200\). Considering the hypergraph partitioning problem is NP-complete, a partitioner that ensures that the optimal solution is found can not possibly be efficient (unless \(P=NP\)). Whenever an extension is enabled, partitioning the hypergraph becomes a more complex task: \texttt{SlideCNOTs} increases the number of vertices reached by each hyperedge, while \texttt{EitherRemote} creates a hypergraph at least twice as large. The problem in cases such as QFT-\(200\) is that one of the extensions (\texttt{EitherRemote} in this case) does exceptionally well, while the other has little impact. Then, when both are enabled, the latter extension essentially makes the partitioner's job more difficult while contributing barely anything and, when the hypergraph is large enough, this results in the partitioner yielding a worse solution. Figure~\ref{fig:results2} makes this evident, as for QFT with dimension \(25\) and lower, enabling both extensions is best, but beyond \(50\) it is not. 

\input{Figures/Results/2}

Still, in all of our tests, enabling any combination of extensions was always better than running the algorithm with none. In practice, we would distribute the circuit with different extensions enabled, and choose the best one. Considering that, in all of our tests, enabling both extensions was always in the top two best configurations, we would propose it as the default configuration to use. This is similar to how classical compilers provide a default set of optimisation flags (e.g.\ \texttt{-O2}), but you may obtain a more efficient object code if you enable/disable some of these flags manually.

\input{Figures/Results/3}

Figure~\ref{fig:results3} follows the same format as in Figure~\ref{fig:results1}, but now the different bars correspond to different values of the parameter \(k\) -- the number of QPUs the circuits are distributed across. For each circuit, its best configuration of extensions is used. As we would expect, distributing the circuit across more QPUs makes more CNOTs non-local and, correspondingly, the ebit count also increases. However, we can see how this increase is not necessarily uniform: When distributing the GSE circuit, the number of non-local gates and ebits required to distribute it across \(10\) QPUs is almost the same as if we distribute across \(7\) QPUs. The opposite situation happens for the BF circuit, as it seems to have a natural way of being partitioned across up to \(7\) QPUs -- possibly because the qubits form groups that, for the most part, work isolated from other groups --, but, beyond \(10\) QPUs, wires that communicate intensively have to be separated -- as a single QPU can not hold all of the qubits in a group. Still, in this particular case our algorithm manages to maintain a low ebit count. 

\input{Figures/Results/4}

Finally, Figure~\ref{fig:results4} shows the ratio between QPU space dedicated to communication (ebit space) versus space dedicated to computation (workspace qubits). A lower ratio is desirable, as we want as much computation space as possible. Naturally, the ratio is worse as we increase the number of QPUs we distribute across, because the workspace required stays the same, while more ebits are needed (as we just saw in Figure~\ref{fig:results3}). Therefore, there is a tension here: we would like to use as many QPUs as possible, but we want them to manage a small space each. Figure~\ref{fig:results4} shows that this tension is different for each circuit: In cases such as BWT and GSE, the ratio barely changes, so if we decide to distribute them, we should use a relatively large amount of QPUs. On the other hand, circuits such as QFT-\(200\) are not good candidates for distribution, as using only \(3\) QPUs already doubles the amount of space needed, and the ratio keeps increasing for larger \(k\).


%QFT does not seem to be prone to distribution. Others, such as gse, seem to be prone for massive distribution, as the drawback is only a payed at the start