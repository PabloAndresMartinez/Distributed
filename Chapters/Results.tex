\chapter{Evaluation}

In the previous chapter we have proposed an algorithm that finds an efficient distribution of any given quantum circuit. Additionally, we have given two extensions (\S\ref{pulCNOTs} and \S\ref{BothEnds}) that allow the algorithm to apply some extra optimisations. In this chapter, we will evaluate the distributed circuits our algorithm generates for a collection of quantum programs. The quantum programs we consider have been discussed in the literature as examples of what we would like to run on quantum computers.

The code of our algorithm's implementation, all the generated executables and all the data discussed in this chapter can be found at: \textbf{TODO}: GitHub. 


\section{Implementation details}
\label{implementation}

The algorithm described in \S\ref{Vanilla} and its two extensions (\S\ref{pulCNOTs} and \S\ref{BothEnds}) have been implemented in Haskell. We chose Haskell because Quipper, a quantum circuit description language, is embedded on it. The quantum programs we use in our evaluation (see \S\ref{benchmark}) are all available as part of the Quipper system, so using Haskell we could easily manage them. Our implementation provides a Quipper circuit as output, and thus it can be integrated in any Quipper program. Besides, the hypergraph partitioning is performed by an specialised third-party software, \texttt{KaHyPart} \citep{KaHyPart}, that is called by our program when needed. A brief overview of \texttt{KaHyPart} is given in Appendix~\ref{HypPart}. 

The main contributions of this thesis were described in Chapter~\ref{chap:Project}. Our implementation is meant to be a demonstration of our algorithm, and it is intended for evaluation purposes only. Thus, efficiency was not a main concern. Nevertheless, it is efficient enough to manage circuits with up to \(300\) wires within a reasonable amount of resources (see \S\ref{Results}).

Our implementation receives a circuit described in Quipper's internal data structure and outputs its distributed version, also in Quipper's format. However, along the process the circuit is managed as a list of gates, rather than using Quipper's internal data structure. This is the main cause of inefficiencies in our code. Quipper is presented to its users as a language to define circuits, rather than a language to define circuit transformations. Therefore, there are not enough functionalities available at user-level to fully implement our algorithm within Quipper. If we intended to achieve better efficiency, we would require to learn the internal workings -- the back-end -- of Quipper, which is beyond the scope of this thesis.

Once our input circuit is converted\footnote{This conversion and its backwards counterpart are provided by Quipper.} from Quipper's data structure to a standard list of gates, implementing our algorithm is straight-forward. Depending on two input flags, the extensions from \S\ref{pulCNOTs} and \S\ref{BothEnds} are applied or not; and the parameters \(k,\epsilon\) -- number of QPUs and load-imbalance tolerance -- are also given as input. The hypergraph generated by Algorithm~\ref{code:buildHypVanilla} (or Algorithm~\ref{code:buildHypBothEnds}) is written in a file using \texttt{KaHyPart}'s format, and the resulting partition is read from \texttt{KaHyPart}'s output file and used to create the distributed circuit as described in \S\ref{Vanilla}.


\section{Test suite}
\label{benchmark}

The quantum programs we will use to evaluate our algorithm are available as part of the Quipper system. On the date this thesis is written, the Quipper system provides seven quantum programs, each with their own default configuration parameters. We use four of these seven examples, with their default configuration unless stated otherwise. The three programs we omit in our evaluation either lack an explicit -- gate by gate -- implementation of some fragment of their circuits, or their size is beyond the capabilities of our implementation (see \S\ref{implementation}). Detailed information about each of these quantum programs can be found in Quipper's online documentation\footnote{Link to Quipper's documentation: \url{https://www.mathstat.dal.ca/~selinger/quipper/doc/}.}. The four programs we consider are the following:

\begin{itemize}
\item \textit{Boolean Formula (BF)}: \citet{BFWalk} showed that the problem of evaluating a boolean formula over \(N\) variables could be solved in time \(\sqrt{N}\) on a quantum computer. We consider the circuit implementing the main part of the algorithm -- the quantum walk. Implemented by A. Green.

\item \textit{Binary Welded Tree (BWT)}: \citet{BWT} proposed the problem of finding a path between two nodes in a particular kind of graph (a binary welded tree), and gave a quantum program that solves it exponentially faster than any known classical algorithm. We consider the circuit implementing the overall algorithm, making the tree height twice as large as the default Quipper's input. Implemented by P. Selinger and B. Valiron.

\item \textit{Ground State Estimation (GSE)}: \citet{GSE} proposed how to efficiently calculate the energy of a molecular system's ground state, which is relevant in chemistry. We consider the circuit implementing the overall algorithm. We doubled the number of basis functions and the number of occupied orbitals. Implemented by A. Green et al.

\item \textit{Unique Shortest Vector (USV)}: \citet{USV} proposed a problem where some characteristic vector of an input lattice must be found. This problem requires a large amount of resources, so we only consider a part of it, labelled `R' in Quipper's library. We reduced the default dimension of the lattice from \(5\) to \(2\). Implemented by N. Ross.
\end{itemize}

Apart from these four programs, we will include in our test suite the circuit for the \textit{Quantum Fourier Transform (QFT)}. The QFT is a key component of many quantum algorithms, Shor's factorisation algorithm being one of them. Its implementation is also provided in Quipper's libraries. In \S\ref{Results} we consider QFT-\(N\), for different values of \(N\), to refer to the QFT circuit of dimension \(N\), which uses \(N+1\) qubits.

\section{Results}
\label{Results}

Our code was compiled using GHC version 8.0.2, with the optimisation flag \texttt{-O2} active. The tests we discuss in this section were run each on a single core of a i7-4710HQ processor, with 8GB of available RAM, on a Ubuntu 16.04 machine. The time it takes to distribute a circuit mainly depends on the number of CNOT gates it has. Table~\ref{tab:time} shows the CNOT count and how long it takes to run the algorithm for each of the tests we discuss in this section. Beyond certain circuit size -- around \(50,000\) CNOTs --, the time it takes is considerably long. This is because our code was not implemented with scalability as a concern, rather than a characteristic of the algorithm itself.

\begin{table}
\caption{CNOT count and the CPU time it takes to distribute each of the circuits discussed in this section. The data shown corresponds to executions with both extensions \texttt{pullCNOTs} and \texttt{EitherRemote} enabled and using distribution parameters \(k\!=\!5,\ \varepsilon\!=\!5\).}
\label{tab:time}
\centering
\vspace*{3mm}
\begin{tabular}{|c|r|r|}
\hline
\textit{Circuit} & \textit{CNOTs} & \textit{Time (s)} \\
\hline
{\small BF} & 25,590 & 64 \\
{\small BWT} & 13,824 & 15 \\
{\small GSE} & 70,158 & 675 \\
{\small USV} & 101,806 & 1273\\
\hline
\end{tabular}
\hspace{10mm}
\begin{tabular}{|c|r|r|}
\hline
\textit{Circuit} & \textit{CNOTs} & \textit{Time} \\
\hline
{\small QFT-\(10\)} & 450 & 0.6 \\
{\small QFT-\(25\)} & 3,000 & 3 \\
{\small QFT-\(50\)} & 12,250 & 17 \\
{\small QFT-\(100\)} & 49,500 & 255 \\
{\small QFT-\(200\)} & 199,000 & \textbf{TODO}\\
\hline
\end{tabular}
\end{table}

Figure~\ref{fig:results1} shows, for different circuits, the proportion of CNOTs that become non-local once the circuit is partitioned. We also display the proportion of ebits the circuit requires. The key idea behind our algorithm was to take advantage of the remote-control and/or remote-target methods to implement multiple CNOTs using a single ebit. The larger the difference between the two proportions shown in Figure~\ref{fig:results1} is, the better this idea has paid off. However, the definitive measure of success -- and what our algorithm aims to optimise -- is the proportion of ebits itself. 

\input{Figures/Results/1}

Each of the four bars shown for every circuit corresponds to a different configuration of extensions enabled (from left to right): the vanilla algorithm, only \texttt{SlideCNOTs} enabled, only \texttt{EitherRemote} enabled and both extensions enabled. Among the four configurations, the best one is that whose proportion of ebits is the lowest\footnote{Notice that for a given circuit, the number of CNOTs in it is independent of the extensions enabled. Thus, the number of ebits is normalised by the same value for each of the four configurations.}. Interestingly, in cases such as BWT when \texttt{SlideCNOTs} is enabled, the algorithm may find a more efficient solution by partitioning the circuit in a way that increases the amount of non-local CNOTs, but ultimately reduces the number of ebits.

As discussed in Remarks~\ref{thm:SlideCNOTsImprove} and~\ref{thm:BothEndsImprove}, in theory, enabling any of the two extensions should never increase the ebit count. However, Figure~\ref{fig:results1} shows that this does not hold in practice. This is because we were assuming that the optimal solution to the hypergraph partitioning problem was always found, but this is not the case in practice. Considering the hypergraph partitioning problem is NP-complete, a partitioner that ensures that the optimal solution is found can not possibly be efficient (unless \(P=NP\)). Whenever an extension is enabled, partitioning the hypergraph becomes a more complex task: \texttt{SlideCNOTs} increases the number of vertices reached by each hyperedge, while \texttt{EitherRemote} creates a hypergraph at least as twice as large. The problem in cases such as QFT-\(200\) is that one of the extensions (\texttt{EitherRemote} in this case) does exceptionally well, while the other has little impact. Then, when both are enabled, the latter extension essentially only makes the partitioner's job more difficult, and when the hypergraph is large enough, this results in the partitioner yielding a worse solution. Figure~\ref{fig:results2} makes this evident, as for QFT with dimension \(25\) and lower, enabling both extensions is best: the hypergraph is small enough for the partitioner to find the optimal solution. 

\input{Figures/Results/2}

Still, in all of our tests, enabling any combination of extensions was always better than running the algorithm with none. In practice, we would distribute the circuit with different extensions enabled, and choose the best one. Considering that, in all of our tests, enabling both extensions was always in the top two best configurations, we would propose it as the default configuration to use. This is similar to how classical compilers provide a default set of optimisation flags (e.g.\ \(-O2\)), but you may obtain a more efficient object code if you disable some of these flags manually.

\input{Figures/Results/3}

Figure~\ref{fig:results3} shows the same information as in Figure~\ref{fig:results1}, but now the different bars correspond to different values of the parameter \(k\) -- the number of QPUs the circuits are distributed across. For each circuit, its best configuration of extensions is used. As we would expect, distributing the circuit across more QPUs makes more CNOTs non-local and, correspondingly, the ebit count also increases. However, we can see how this increase is not necessarily uniform: When distributing the GSE circuit, the number of non-local gates and ebits required to distribute it across \(10\) QPUs is almost the same as if we distribute across \(7\) QPUs. The opposite situation happens for the BF circuit, as it seems to have a natural way of being partitioned across up to \(7\) QPUs -- possibly because the qubits form groups that, for the most part, work isolated from other groups --, but beyond \(10\) QPUs wires that communicate intensively have to be separated -- as a single QPU can not hold all of the qubits in a group. Still, in this particular case our algorithm manages to maintain a low ebit count. 

\input{Figures/Results/4}

Finally, Figure~\ref{fig:results4} shows the ratio between QPU space dedicated to communication (ebit space) versus space dedicated to computation (workspace qubits). A lower ratio is more desirable, as we want as much computation space as possible. Naturally, the ratio is worse as we increase the number of QPUs we distribute across, because the workspace required stays the same, while more ebits are needed (as we just saw in Figure~\ref{fig:results3}). Therefore, there is a tension here, as we would like to use as many QPUs as possible, but we want them to manage a small space each. Figure~\ref{fig:results4} shows that this tension is different for each circuit: In cases such as BWT and GSE, the ratio barely changes, so if we decide to distribute them, we should use a large amount of QPUs. On the other hand, circuits such as QFT-\(200\) are not good candidates for distribution, as using only \(3\) QPUs already doubles the amount of space needed, and the ratio keeps increasing for larger \(k\).


%QFT does not seem to be prone to distribution. Others, such as gse, seem to be prone for massive distribution, as the drawback is only a payed at the start