\chapter{Quantum Computing: A brief overview}

Quantum computing aims to take advantage of quantum mechanics to speed-up computations. There is strong theoretical evidence that quantum computers are capable of solving some problems substantially faster than standard (classical) computers. Well known examples are:

\begin{itemize} 
  \item Shor's algorithm for \textit{polynomial-time} large number factorisation~\citep{Shor}. There is no known algorithm on classical computers that can perform this task in polynomial time, and it is suspected not to be possible\footnote{RSA, a widely used encryption system, relies its security on the assumption that factorisation of large numbers can not be computed efficiently.}. Quantum computers theoretically provide an exponential speed-up on this task.
  \item Grover's algorithm for efficient unstructured search~\citep{Grover}. The algorithm performs a brute-force search (i.e.\ requiring no knowledge about the search space) over a \(N\) data-points in time proportional to \(\sqrt{N}\). A brute-force search in a classical computer should always take time proportional to \(N\).
\end{itemize}

Besides, in May of the present year, Raz and Tal~\citep{BQPSepPH} gave formal proof of the existence of problems that a classical computer may never solve in polynomial time, but are solvable in polynomial time on a quantum one. Nevertheless, all of these results are theoretical in nature and there are some caveats on their practical implementation, discussed in \S~\ref{Challenges}. Giving experimental evidence of this time-efficiency separation between quantum and classical computers is a highly active area of research, known as \textit{quantum supremacy}. 

Quantum computing would be very valuable in many areas of research where classical computers are unable to solve problems efficiently. Some of the main applications that have been discussed in the literature are:

\begin{itemize}
  \item \textit{Chemistry, medicine and material sciences}: Calculating molecular properties on complex systems is an intractable problem for classical computers. However, polynomial algorithms for this problem are known for quantum computers~\citep{TowardsQuantumChemistry}. Hence, quantum computers are likely to trigger a revolution on areas of science that need to model molecules and their interactions.
  \item Machine learning: Finding patterns in a large pool of data is the essence of machine learning. Multiple quantum algorithms have been shown to be able to detect patterns that are believed not to be efficiently attainable classically~\citep{QuantumMachineLearning}.
  \item Engineering: Optimization and search problems are common in almost every area of engineering. Quantum computers are particularly well suited for these tasks, with Grover's algorithm being a clear example.
\end{itemize}

For any of these applications we will require large scale quantum computers. Due to the obstacles in the way of building a large mainframe quantum computer (see \S~\ref{Challenges}), some authors have advocated the alternative of building a quantum multicomputer: a grid of smaller quantum computer units that cooperate in performing an overall computation~\citep{DistributedQCHW}. In the present work, particularly in Chapter~\ref{Project}, we contribute to this perspective, providing a method for efficiently distributing any quantum program originally designed for a monolithic quantum computer.

\section{The principles of Quantum Computing}
\label{Principles}

The advantage of using quantum mechanics to perform computations is usually traced down to the following three principles:

\begin{itemize}

  \item \textit{Superposition}: In classical computing, the unit of information is the \textit{bit}, which may take one of two values: \(0\) or \(1\). In quantum computing, the \textit{bit}'s counterpart is the \textit{qubit}, whose value may be \textit{any linear combination} of \(0\) and \(1\), known as a \textit{superposition}, and usually written as: \[\ket{qubit} = \alpha\ket{0} + \beta\ket{1}\] where \(\alpha\) and \(\beta\) are complex numbers that must satisfy: \(\alpha^2 + \beta^2 = 1\).

  A popular analogy of a qubit's superposition is a coin spinning\footnote{Note this is just an analogy, and while a coin spinning can be perfectly modelled using classical physics, a qubit can not. In fact, superposition is key in the (even weirder) two other principles that differentiate quantum and classical computing.
  }: the classical states (\(0\) and \(1\)) are \textit{heads} and \textit{tails}, but when the coin is spinning, its state is neither of them. If we knew exactly how the coin was set spinning, we would be able to describe the probability distribution of seeing heads or tails when it stops; these would be our \(\alpha^2\) and \(\beta^2\) values. We may \textit{measure} a qubit, and doing so corresponds in our analogy to stopping the coin: we will get either \(0\) or \(1\) as outcome.

  The essential aspect of this analogy is that, before measurement, the \textit{qubit}'s state is neither \(0\) nor \(1\). Through certain operations (that would correspond to altering the axis of spin of the coin), we may change the coefficients \(\alpha\) and \(\beta\) of the superposition. Interestingly, in quantum computing, we encode input and read output (after measurement) as standard classical binary strings, and thus, \textit{for input/output we use as many qubits as bits would be required}. What superposition provides is the ability to -- during mid-computation -- maintain a superposition of all potential solutions to the problem, and update all of them simultaneously with a single operation to the qubits. In some sense, superposition allows us to explore multiple choices/paths of the computation, using only the resources required to explore a single one of those paths. And the number of path we can explore simulatenously can be up to exponential in comparison to the classical case, as a string of \(N\) qubits may be in a state of superposition of all the \(2^N\) classical states. This is the reason behind the exponential speed-up of Shor's algorithm.

  \item \textit{Interference}: As we just discussed, superposition gives us the ability to simultaneously explore different paths to solve a problem. However, in the end we will need to measure the qubits -- stop the coins, in order to read heads or tails -- and the result will be intrinsically random. For quantum computing to be any better than a probabilistic classical computer, we require the ability to prune the paths that have led to a dead-end. This is precisely what \textit{interference} provides: some operations on the qubits may make different classical states in the superposition cancel each other out. Interference is at the core of any speed-up achieved by a quantum algorithm, and taking advantage of it is the main challenge when designing quantum algorithms.

  \item \textit{Entanglement}: Quantum mechanics allows us to have a pair of qubits \(a\) and \(b\) in a superposition such as: \[\ket{a,b} = \frac{1}{\sqrt{2}}\ket{0,0} + \frac{1}{\sqrt{2}}\ket{1,1}\] This implies that, when we measure the qubits, we may only read \(a=0, b=0\) or \(a=1, b=1\) as outcome (the coefficients for \(\ket{0,1}\) and \(\ket{1,0}\)) are both \(0\)). Then, what happens if we only measure \(a\)? In this case, we would also know \(b\)'s outcome, without measuring it. More surprisingly, if we measured both \(a\) and \(b\) at the same instant, we would obtain \(a=0, b=0\) half of the times and \(a=1, b=1\) the other half. In short, it seems like acting on one qubit has an instantaneous effect on the other. Whenever a group of qubits exhibits this property, we say they are \textit{entangled}. Entanglement holds regardless how far apart \(a\) is from \(b\); for instance, they could be on two different quantum processing units of a distributed grid. Indeed, entanglement will be key in our discussion of distributed quantum algorithms, and we explain how to use it to perform non-local operations in \S~\ref{IntroDistributing}.

\end{itemize}


\section{Building Quantum Computers}
\label{Hardware}

Physicists have come up with different ways of realising qubits in labs. The key idea is to find a physical system that displays non-classical behaviour, and put it under the appropriate circumstances so we can manage its quantum properties, but noise in the environment may not interfere with these. \citet{ArchitectureSurvet} gives an excellent survey of the state of the art of quantum architectures. Among them, the three closest to experimental realisation are:

\begin{itemize}

  \item \textit{Optics}: The state of a qubit is represented in the properties of photons, for instance, their polarization~\citep{OpticsQC}. A great advantage of this technology is that photons can be easily sent over long distances, while preserving the quantum state. Thus, protocols in quantum information that heavily rely on communication, such as Quantum Key Distribution~\citep{QKD}, are usually discussed and experimented using optics. The downside of optics is that it is very difficult to make photons interact, which is required for other than single qubit operations.

  \item \textit{Ion-traps}: Each qubit is embodied as an ion, confined inside a chamber by means of an electric or magnetic fields. The qubit is acted upon by hitting the ion with electromagnetic pulses (e.g.\ laser light or microwave radiation). This is one of the most promising technologies for the future of quantum computing, with groups of experimentalists having proposed how to scale up the technology~\citep{HensingerIonTraps}.

  \item \textit{Superconductors}: Small circuits, similar to classical electrical circuits, are cooled down to near absolute zero so the quantum interactions of electrons are not obscured by other perturbations. Then, different parts of the circuit encode different qubits, which can be acted upon by applying different electric potentials. One of the main advantages of this technology is that the challenges for scaling up such circuits, apart from the cooling system, are similar to the challenges we have encountered over the years for classical computers. Therefore, this technology seems to be the most feasible in the near future and evidence of that is the fact that, using it, both IBM and Intel have already built small generic-purpose quantum computers of 17-20 qubits.

\end{itemize}

However, for quantum computers to be useful in real world applications, their qubit count should raise up, at the very least, one order of magnitude. And, unfortunately, increasing the amount of qubits in a quantum computer is particularly difficult, due to some caveats we will now discuss.


\subsection{Scalability challenges}
\label{Challenges}

There are two main challenges to overcome in order to build large scale quantum computers:

\begin{itemize}

  \item \textit{Decoherence}: In \S~\ref{Principles} we discussed the importance of having superposition in quantum computing, and we compared a qubit in superposition with a coin spinning. For the same reason why a coin spinning will eventually stop, a qubit in superposition will eventually degenerate into a classical state (i.e.\ either \(\ket{0}\) or \(\ket{1}\)): physical systems have a tendency towards the state at which they are most stable, for the coin it is laying flat, for the qubit it is losing its superposition. This phenomenon is known as \textit{decoherence} and it will always occur in any given technology\footnote{A revolutionary technology, \textit{anyonic} (a.k.a.\ topological) quantum computing, has been proposed to theoretically avoid the problem of decoherence by using physical systems that can be completely protected against it~\citep{Anyonic}. Although promising, currently this proposal has little experimental underpinning, and it is not regarded as attainable in the near future.}, in some faster than others. Experimentalists attempt to increase the time it takes for the state of the qubit to degenerate, which in both ion-trap and superconductor technologies it is in the order of microseconds. Decoherence constitutes the main constraint to scalability of quantum computers, as it dictates the lifespan of qubits, limiting the number of operations that can be applied in a single program. 

  Certainly, the state of bits also degenerates in classical computers. However, in their case this is easier to account for: intuitively, we can keep monitoring the bits, and make sure to correct any unwanted change. This is not so simple in quantum computers, as monitoring a qubit would require \textit{measuring} it, and that destroys any quantum superposition. Nevertheless, it is still to some extent possible to protect our quantum state from errors -- either due to decoherence or imperfect hardware -- through quantum error correction routines~\citep{QuantumErrorCorrection}. This is a very active area of research, and it will be key for the implementation of reliable large scale quantum computers.


  \item \textit{Connectivity}: In order to run complex computations on qubits, we will need to be able to apply apply multi-qubit operations on any subset of the available qubits. However, it is not realistic to expect that quantum computers will have fast connectivity between all qubits, due to spatial separation of these in the hardware. In classical systems, this problematic is solved by a memory hierarchy, with a ceaseless flow of data going up and down of it, from main memory to registers and back. However, the memory hierarchy model works because most of the data can stay idly in main memory while computation on the registers data is carried out. In quantum computers, we must avoid qubits being idle, as decoherence prevents the existence long-lasting memory. An alternative found in classical computers is to distribute the computation across different processing units, each having its own local memory which they use intensively, and communicating -- through message passing -- as little as possible. In \S~\ref{DQC_Architecture}, we discuss an abstract distributed quantum architecture in detail.

\end{itemize}


\subsection{Models of computation}
\label{Models}

In this section, we give a brief introduction to some models of quantum computation relevant to the this thesis.

\begin{itemize}

  \item \textit{Circuit model}: Also known as the network model. Any operation on \(n\) qubits -- as long as measurement (i.e.\ destruction of information) is not involved -- can be represented as square matrix on complex numbers, of dimension \(2^n\). These matrices are always unitary, which means that a matrix \(U\)satisfies \(UU^\dag = I = U^\dag U\), where \(I\) is the identity matrix and \(A^\dag\) is the conjugate transpose of \(A\). Essentially, unitarity ensures that any operation on qubits can be reversed (i.e.\ undone), reason why this model is sometimes called the reversible model. Multiplying matrices \(AB\) corresponds to applying the operation described by \(B\) first, then \(A\), on the same qubits. Application of two operations on disjoint set of qubits corresponds to the Kronecker product of the matrices \(A \otimes B\). The fundamental concept is that any matrix can be represented as a product of other matrices, so we may decompose any operation into smaller building blocks: quantum gates.

  Qubits are pictured as wires to which quantum gates are applied, similarly to a classical digital circuit. The set of quantum gates used is dependent on the architecture. There exist an (uncountable) infinite amount of different quantum operations, but a small finite set of them is enough to approximate any of them, up to a desired error factor. The most common choice of such a universal gate-set is \texttt{Clifford+T}, which contains six one-qubit gates, and a single two-qubit gate. The depiction of such gates and some of their most important properties are shown in Figures~\textbf{TODO}. Circuits are read from left to right.

  \textbf{TODO:} A figure depicting, and giving the matrix of, each of CNOT, X, Y, Z, H, S and T. Figures showing HXH = Z; XX = I, ZZ = I, YY = I, HH = I; SS = Z; TT = S; H2 CNOT H2 = NOTC, and also their algebraic notation.

  The CNOT gate (Figure~\ref{fig:CNOT}) is particularly interesting. The qubit where the filled dot is acts as the `control', and the qubit with \(\oplus\) acts as `target'. Whenever the control is \(\ket{0}\), the target is unaffected; but if it is \(\ket{1}\), an X gate (Figure~\ref{fig:X}) is applied, flipping the value of the qubit. This works in any superposition, so if in \[\ket{c,t} = \alpha\ket{0,0} + \beta\ket{0,1} + \gamma\ket{1,0} + \delta\ket{1,1}\] \(\ket{c}\) were acting as control and \(\ket{t}\) as target, the outcome would be: \[CNOT \ket{c,t} = \alpha\ket{0,0} + \beta\ket{0,1} + \gamma\ket{1,1} + \delta\ket{1,0}\]

  \textbf{TODO:} A figure showing an abstract quantum circuit.

  \item \textit{MBQC model}: Initials stand for Measurement Based Quantum Computing. Unlike the circuit model, where measurements are done at the very end of the circuit, MBQC carries out computations by means of repeatedly measuring an initial entangled resource. The process, sketched in Figure~\ref{fig:MBQC} can be thought of as sculpting a statue from a block of granite. The initial entangled resource, which is a collection of entangled qubits forming a lattice structure, corresponds to the granite block. By measuring some qubits in the lattice -- hitting the rock with a chisel -- we remove some of the excess qubits, changing the overall state in the process. The outcome of measurements is probabilistic so, in order to provide deterministic computation, we must apply corrections on the neighbouring qubits whenever the measurement outcome deviated from the desired result. After multiple iterations of measurements and corrections, we end up with a set of qubits encoding the result. The input was incorporated into the lattice at the beginning of the process.

  \textbf{TODO:} Figure from slides %\label{fig:MBQC}

  In this way, any computation may be performed by applying 1-qubit measurement and 1-qubit correcting gates (controlled by classical signals). The initial resource state contains all the entanglement that is required, which may be prepared experimentally through multi-qubit interactions, such as Ising interactions~\citep{1WQC}, which are within our experimental capabilities. Hence, in some sense this model solves the problem of connectivity by applying a single large operation at the beginning of the process, and then only requiring cheap single qubit operations. The main drawback of MBQC is the large amount of qubits that are required for even the simplest of operations, but given this might be surmountable considering the rest of the architecture is greatly simplified. The MBQC model was presented for the first time by Raussendorf and Briegel~\citep{1WQC} under the name of \textit{one-way quantum computer}, highlighting its main difference with the circuit (reversible) approach. 

  \item \textit{Distributed model}: We may find a balance between the circuit model and MBQC, where multiple small quantum processing units run fragments of the overall circuit, and communication is achieved through a shared entangled resource. This model has been discussed in detail in the literature~\citep{DistributedQCHW} and it is at the core of the Networked Quantum Information Technologies Hub (NQIT)\footnote{a project supported by the UK National Quantum Technology program, aiming to provide scalable quantum computing}. In \S~\ref{DQC_Architecture}, we discuss an abstract distributed quantum architecture in detail.

\end{itemize}


\section{Programming on Quantum Computers}

As of today, most quantum programming languages are high level circuit descriptors: they provide the means to define circuits gate by gate, or build them up from combinations of smaller circuits. In this category fall all the well-known languages, such as \textit{QCL}~\citep{QCL} (imperative paradigm, and one of the first quantum programming languages ever implemented), \textit{Q\#}~\citep{QLang} (imperative, designed by Microsoft), and \textit{Quipper}~\citep{Quipper} (functional, built on top of Haskell). Besides, there are attempts at designing quantum programming languages that are completely hardware agnostic, meaning they aim to describe the computation, rather than a particular circuit that implements it. Examples of these are the different attempts at defining a quantum lambda calculus, for instance van Tonder's~\citep{VanTonder} or Diaz-Caro's~\citep{Diaz-Caro}). However, these are still early in their development and tend to be particularly verbose.

Additionally, most of the literature on quantum algorithms describes these by explicitly giving circuits that implement them. There is a constructive procedure, given by the Solovay-Kitaev theorem~\citep{SolovayKitaev}, that takes any circuit and a choice of universal gate-set and outputs an efficient equivalent circuit using only those gates. Hence, programmers do not need to worry about the gates they are using when describing their circuits.

Unfortunately, the fact that algorithms are almost exclusively defined in the circuit model implies that other models of quantum computing (introduced in \S~\ref{Models}) are disregarded by a large portion of the community. In order to make other models of computation accessible, we need to provide automated procedures for transforming algorithms from the circuit models to these (and vice versa). Work has been done on the transformation from circuit to MBQC and backwards, the latter being the most challenging~\citep{gflow}. However, there is little amount of literature describing how to go from the circuit model to the distributed model. In \S~\ref{IntroDistributing} we give an overview of the existent work on that aspect, and identify the gap on the literature we aim to answer in this thesis.


\section{Summary}

As a wrap up, here are the key concepts to keep in mind while reading the rest of this thesis:

\begin{itemize}
  \item Quantum computers provide a computing power well beyond the capabilities of classical computers, which would be exploitable in many areas of science.
  \item Small quantum computers are already available. 
  \item Scaling up is a challenging problem due to: \textit{decoherence}, which may be overcome by the joint effort of error-correction, physics and engineering communities; and \textit{connectivity}, which may be solved using distributed architectures.
  \item There is practically no programming support for distributed architectures.
\end{itemize}