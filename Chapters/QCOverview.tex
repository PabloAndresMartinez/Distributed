\chapter{Quantum Computing: A brief overview}

Quantum computing aims to take advantage of quantum mechanics to speed-up computations. There is strong theoretical evidence that quantum computers are capable of solving some problems substantially faster than standard (classical) computers. Well known examples are:

\begin{itemize} 
  \item Shor's algorithm for \textit{polynomial-time} large number factorisation~\cite{Shor}. There is no known algorithm on classical computers that can perform this task in polynomial time, and it is suspected not to be possible\footnote{RSA, a widely used encryption system, relies its security on the assumption that factorisation of large numbers can not be computed efficiently.}. Quantum computers theoretically provide an exponential speed-up on this task.
  \item Grover's algorithm for efficient unstructured search~\cite{Grover}. The algorithm performs a brute-force search (i.e.\ requiring no knowledge about the search space) over a \(N\) data-points in time proportional to \(\sqrt{N}\). A brute-force search in a classical computer should always take time proportional to \(N\).
\end{itemize}

Besides, in May of the present year, Raz and Tal~\cite{BQPSepPH} gave formal proof of the existence of problems that a classical computer may never solve in polynomial time, but are solvable in polynomial time on a quantum one. Nevertheless, all of these results are theoretical in nature and there are some caveats on their practical implementation, discussed in \S~\ref{Challenges}. Giving experimental evidence of this time-efficiency separation between quantum and classical computers is a highly active area of research, known as \textit{quantum supremacy}~\cite{QSupremacySurvey}. The general opinion of the scientific community is that we will eventually overcome these caveats, and take advantage of quantum computing in fields of science such as:

\begin{itemize}
  \item Chemistry: \textbf{TODO}
  \item Machine learning: \textbf{TODO}
  \item Engineering: \textbf{TODO}
\end{itemize}

For these applications, we will require large scale quantum computers. Due to the obstacles in the way of building a large mainframe quantum computer (see \S~\ref{Challenges}), some authors have advocated the alternative of building a grid of smaller quantum computer units that cooperate in performing an overall computation~\cite{DistributedQCHW}. In this work, particularly in Chapter~\ref{Project}, we contribute to this perspective, providing a method for efficiently distributing any quantum program originally designed for a monolithic quantum computer.

\section{The principles of Quantum Computing}
\label{Principles}

The advantage of using quantum mechanics to perform computations is usually traced down to the following three principles:

\begin{itemize}

  \item \textit{Superposition}: In classical computing, the unit of information is the \textit{bit}, which may take one of two values: \(0\) or \(1\). In quantum computing, the \textit{bit}'s counterpart is the \textit{qubit}, whose value may be \textit{any linear combination} of \(0\) and \(1\), known as a \textit{superposition}, and usually written as: \[\ket{qubit} = \alpha\ket{0} + \beta\ket{1}\] where \(\alpha\) and \(\beta\) are complex numbers that must satisfy: \(\alpha^2 + \beta^2 = 1\).

  A popular analogy of a qubit's superposition is a coin spinning\footnote{Note this is just an analogy, and while a coin spinning can be perfectly modelled using classical physics, a qubit can not. In fact, superposition is key in the (even weirder) two other principles that differentiate quantum and classical computing.
  }: the classical states (\(0\) and \(1\)) are \textit{heads} and \textit{tails}, but when the coin is spinning, its state is neither of them. If we knew exactly how the coin was set spinning, we would be able to describe the probability distribution of seeing heads or tails when it stops; these would be our \(\alpha^2\) and \(\beta^2\) values. We may \textit{measure} a qubit, and doing so corresponds in our analogy to stopping the coin: we will get either \(0\) or \(1\) as outcome.

  The essential aspect of this analogy is that, before measurement, the \textit{qubit}'s state is neither \(0\) nor \(1\). Through certain operations (that would correspond to altering the axis of spin of the coin), we may change the coefficients \(\alpha\) and \(\beta\) of the superposition. Interestingly, in quantum computing, we encode input and read output (after measurement) as standard classical binary strings, and thus, \textit{for input/output we use as many qubits as bits would be required}. What superposition provides is the ability to -- during mid-computation -- maintain a superposition of all potential solutions to the problem, and update all of them simultaneously with a single operation to the qubits. In some sense, superposition allows us to explore multiple choices/paths of the computation, using only the resources required to explore a single one of those paths.

  \item \textit{Interference}: Superposition gives us the ability to simultaneously explore different paths to solve a problem. However, in the end we will need to measure the qubits -- stop the coins, in order to read heads or tails -- and the result will be intrinsically random. For quantum computing to be any better than a probabilistic classical computer, we require the ability to prune the paths that have led to a dead-end. This is precisely what \textit{interference} provides: some operations on the qubits may make different classical states in the superposition cancel each other out. Interference is at the core of any speed-up achieved by a quantum algorithm, and taking advantage of it is the main challenge when designing quantum algorithms.

  \item \textit{Entanglement}: Quantum mechanics allows us to have a pair of qubits \(a\) and \(b\) in a superposition such as: \[\ket{a,b} = \frac{1}{\sqrt{2}}\ket{0,0} + \frac{1}{\sqrt{2}}\ket{1,1}\] This implies that, when we measure the qubits, we may only read \(a=0, b=0\) or \(a=1, b=1\) as outcome (the coefficients for \(\ket{0,1}\) and \(\ket{1,0}\)) are both \(0\)). Then, what happens if we only measure \(a\)? In this case, we would also know \(b\)'s outcome, without measuring it. More surprisingly, if we measured both \(a\) and \(b\) at the same instant, we would obtain \(a=0, b=0\) half of the times and \(a=1, b=1\) the other half. In short, it seems like acting on one qubit has an instantaneous effect on the other. Whenever a group of qubits exhibits this property, we say they are \textit{entangled}. Entanglement holds regardless how far apart \(a\) is from \(b\); for instance, they could be on two different quantum processing units of a distributed grid. Indeed, entanglement will be key in our discussion of distributed quantum algorithms, and we explain how to use it to perform non-local operations in \S~\ref{NonLocalGates}.

\end{itemize}


\section{Building Quantum Computers}

Physicists have come up with different ways of realising qubits in labs. The key idea is to find a physical system that displays non-classical behaviour, and put it under the appropriate circumstances so we can manage its quantum properties, but noise in the environment may not interfere with these. Examples of such successful technologies are:

\begin{itemize}

  \item \textit{Photonics}: The state of a qubit is represented in the properties of photons, for instance, their polarization~\cite{PolarizationQC}. A great advantage of this technology is that photons can be easily sent over long distances, while preserving the quantum state. Thus, protocols in quantum information that heavily rely on communication, such as Quantum Key Distribution~\cite{QKD}, are usually discussed and experimented with using photonics. The downside of photonics is that it is inadequate for building general-purpose quantum computers, as the technology is not scalable.

  \item \textit{Ion-traps}: Each qubit is embodied as an ion, confined inside a chamber by means of an electric or magnetic fields. The qubit is acted upon by hitting the ion with electromagnetic pulses (e.g.\ laser light or microwave radiation). This is one of the most promising technologies for the future of quantum computing, with groups of experimentalists having proposed how to scale up the technology~\cite{HensingerIonTraps}.

  \item \textit{Superconductors}: Small circuits, similar to classical electrical circuits, are cooled down to near absolute zero so the quantum interactions of electrons are not obscured by other perturbations. Then, different parts of the circuit encode different qubits, which can be acted upon by applying different electric potentials. One of the main advantages of this technology is that the challenges for scaling up such circuits, apart from the cooling system, are similar to the challenges we have encountered over the years for classical computers. Therefore, this technology seems to be the most feasible in the near future and evidence of that is the fact that, using it, both IBM~\cite{IBM_Superconductor} and Intel\cite{Intel_Superconductor} have already built small generic-purpose quantum computers of 17-20 qubits.

\end{itemize}

However, for quantum computers to be useful in real world applications, their qubit count should raise up, at the very least, one order of magnitude. And, unfortunately, increasing the amount of qubits in a quantum computer is particularly difficult, due to some caveats we will now discuss.


\subsection{Scalability challenges}
\label{Challenges}

There are two main challenges to overcome in order to build large scale quantum computers:

\begin{itemize}

  \item \textit{Decoherence}: In \S~\ref{Principles} we discussed the importance of having superposition in quantum computing, and we compared a qubit in superposition with a coin spinning. For the same reason why a coin spinning will eventually stop, a qubit in superposition will eventually degenerate into a classical state (i.e.\ either \(\ket{0}\) or \(\ket{1}\)): physical systems have a tendency towards the state at which they are most stable, for the coin it is laying flat, for the qubit it is losing its superposition. This phenomenon is known as \textit{decoherence} and it will always occur in any given technology\footnote{Some experts have discussed that encoding qubits in \textit{Majorana fermions} would avoid decoherence~\cite{Majorana}. Although promising, currently this proposal has little experimental underpinning, and it is not regarded as attainable in the near future.}, in some faster than others. Experimentalists attempt to increase the time it takes for the state of the qubit to degenerate, which in both ion-trap and superconductor technologies it is in the order of microseconds. There has been an steady improvement of this decoherence time across the years~\cite{SurveyDecoherence}, and it is expected to keep improving. Nevertheless, it remains the main constraint to scalability of quantum computers, as it dictates the lifespan of qubits, limiting the number of operations that can be applied in a single program. 

  Certainly, the state of bits also degenerates in classical computers. However, in their case this is easier to account for: intuitively, we can keep monitoring the bits, and make sure to correct any unwanted change. This is not so simple in quantum computers, as monitoring a qubit would require \textit{measuring} it, and that destroys any quantum superposition. Nevertheless, it is still to some extent possible to protect our quantum state from errors -- either due to decoherence or imperfect hardware -- through quantum error correction routines~\cite{QuantumErrorCorrection}. This is a very active area of research, and it will be key for the implementation of reliable large scale quantum computers.


  \item \textit{Connectivity}: In order to run complex computations on qubits, we will need to be able to apply apply multi-qubit operations on any subset of the available qubits. However, it is not realistic to expect that quantum computers will have fast connectivity between all qubits, due to spatial separation of these in the hardware. In classical systems, this problematic is solved by a memory hierarchy, with a ceaseless flow of data going up and down of it, from main memory to registers and back. However, the memory hierarchy model works because most of the data can stay idly in main memory while computation on the registers data is carried out. In quantum computers, we must avoid qubits being idle, as decoherence prevents the existence long-lasting memory. An alternative found in classical computers is to distribute the computation across different processing units, each having its own local memory which they use intensively, and communicating -- through message passing -- as little as possible. In \S~\ref{DQC_Architecture}, we discuss an abstract distributed quantum architecture in detail.

\end{itemize}


\subsection{Models of computation}

In this section, we give a brief introduction to some models of quantum computation relevant to the this thesis.

\begin{itemize}

  \item \textit{Circuit model}: Also known as the network model. Any operation on \(n\) qubits -- as long as measurement (i.e.\ destruction of information) is not involved -- can be represented as square matrix on complex numbers, of dimension \(2^n\). These matrices are always unitary, which means that a matrix \(U\)satisfies \(UU^\dag = I = U^\dag U\), where \(I\) is the identity matrix and \(A^\dag\) is the conjugate transpose of \(A\). Essentially, unitarity ensures that any operation on qubits can be reversed (i.e.\ undone), reason why this model is sometimes called the reversible model. Multiplying matrices \(AB\) corresponds to applying the operation described by \(B\) first, then \(A\), on the same qubits. Application of two operations on disjoint set of qubits corresponds to the Kronecker product of the matrices \(A \otimes B\). The fundamental concept is that any matrix can be represented as a product of other matrices, so we may decompose any operation into smaller building blocks: quantum gates.

  Qubits are pictured as wires to which quantum gates are applied, similarly to a classical digital circuit. The set of quantum gates used is dependent on the architecture. There exist an (uncountable) infinite amount of different quantum operations, but a small finite set of them is enough to approximate any of them, up to a desired error factor. The most common choice of such a universal gate-set is \texttt{Clifford+T}, which contains six one-qubit gates, and a single two-qubit gate. The depiction of such gates and some of their most important properties are shown in Figures~\textbf{TODO}.

  \textbf{TODO:} A figure depicting, and giving the matrix of, each of CNOT, X, Y, Z, H, S and T. Figures showing HXH = Z; XX = I, ZZ = I, YY = I, HH = I; SS = Z; TT = S; H2 CNOT H2 = NOTC, and also their algebraic notation.

  \textbf{TODO:} A figure showing an abstract quantum circuit.

  \item \textit{MBQC model}: Initials stand for Measurement Based Quantum Computing. Unlike the circuit model, where measurements are done at the very end of the circuit, MBQC carries out computations by means of repeatedly measuring an initial entangled resource. The process, sketched in Figure~\ref{fig:MBQC} can be thought of as sculpting a statue from a block of granite. The initial entangled resource, which is a collection of entangled qubits forming a lattice structure, corresponds to the granite block. By measuring some qubits in the lattice -- hitting the rock with a chisel -- we remove some of the excess qubits, changing the overall state in the process. The outcome of measurements is probabilistic so, in order to provide deterministic computation, we must apply corrections on the neighbouring qubits whenever the measurement outcome deviated from the desired result. After multiple iterations of measurements and corrections, we end up with a set of qubits encoding the result. The input was incorporated into the lattice at the beginning of the process.

  \textbf{TODO:} Figure from slides %\label{fig:MBQC}

  In this way, any computation may be performed by applying 1-qubit measurement and 1-qubit correcting gates (controlled by classical signals). The initial resource state contains all the entanglement that is required, which may be prepared experimentally through multi-qubit interactions, such as Ising interactions~\cite{1WQC}, which are within our experimental capabilities. Hence, in some sense this model solves the problem of connectivity by applying a single large operation at the beginning of the process, and then only requiring cheap single qubit operations. The main drawback of MBQC is the large amount of qubits that are required for even the simplest of operations, but given this might be surmountable considering the rest of the architecture is greatly simplified. The MBQC model was presented for the first time by Raussendorf and Briegel~\cite{1WQC} under the name of \textit{one-way quantum computer}, highlighting its main difference with the circuit (reversible) approach. 

  \item \textit{Distributed model}: We may find a balance between the circuit model and MBQC, where multiple small quantum processing units run fragments of the overall circuit, and communication is achieved through a shared entangled resource. This model has been discussed in detail in the literature~\cite{RealDQC} and it is at the core of the Networked Quantum Information Technologies (NQIT) project~\cite{NQIT}. In \S~\ref{DQC_Architecture}, we discuss an abstract distributed quantum architecture in detail.

\end{itemize}


\section{Programming on Quantum Computers}



Very low level and fixed on circuit model!



\section{Summary}

\textbf{TODO}

Here are the key concepts to keep while reading the rest of this thesis:

Quantum computers are great
Small quantum computers are available
Challenges to scalability: decoherence (potentially solved by engineering and error correcting) and connectivity (potentially solved by distributed architectures)
No programming support for distributed architectures